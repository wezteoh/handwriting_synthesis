{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import sys\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pytorch modules\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.utils.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting\n",
    "sys.path.insert(0,'..')\n",
    "from utils import plot_stroke"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find gpu\n",
    "cuda = torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparamters\n",
    "timesteps = 800\n",
    "num_clusters = 20\n",
    "cell_size = 400\n",
    "nlayers = 2\n",
    "bsize = 150\n",
    "init_lr = 1E-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare training data\n",
    "train_data = [np.load('train_strokes_800.npy'), np.load('train_masks_800.npy'), np.load('train_onehot_800.npy')]\n",
    "for _ in range(len(train_data)):\n",
    "    train_data[_] =torch.from_numpy(train_data[_]).type(torch.FloatTensor)\n",
    "    if cuda:\n",
    "        train_data[_] = train_data[_].cuda()\n",
    "train_data = [(train_data[0][i], train_data[1][i], train_data[2][i]) for i in range(len(train_data[0]))] \n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    train_data, batch_size=bsize, shuffle=True)\n",
    "    \n",
    "# prepare validation data\n",
    "validation_data = [np.load('validation_strokes_800.npy'), np.load('validation_masks_800.npy'), \n",
    "                   np.load('validation_onehot_800.npy')]\n",
    "for _ in range(len(validation_data)):\n",
    "    validation_data[_] = torch.from_numpy(validation_data[_])\n",
    "    if cuda:\n",
    "        validation_data[_] = validation_data[_].cuda()\n",
    "validation_data = [(validation_data[0][i], validation_data[1][i], validation_data[2][i]) \n",
    "                   for i in range(len(train_data[0]))] \n",
    "validation_loader = torch.utils.data.DataLoader(\n",
    "    validation_data, batch_size=bsize, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2-layer lstm with mixture of gaussian parameters as outputs\n",
    "# skip connections added\n",
    "class LSTM(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LSTM, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_size = 3, hidden_size = cell_size, num_layers = 1, batch_first=True)\n",
    "        self.lstm2 = nn.LSTM(input_size = cell_size+3, hidden_size = cell_size, num_layers = 1, batch_first=True)\n",
    "        self.linear1 = nn.Linear(cell_size*2, 1+ num_clusters*6)\n",
    "        self.tanh = nn.Tanh()\n",
    "        \n",
    "    def forward(self, x, prev, prev2):\n",
    "        timesteps = x.shape[1]\n",
    "        h1, (h1_n, c1_n) = self.lstm(x, prev)\n",
    "        x2 = torch.cat([h1, x], dim=-1)\n",
    "        h2, (h2_n, c2_n) = self.lstm2(x2, prev2)\n",
    "        h = torch.cat([h1, h2], dim=-1)\n",
    "        params = self.linear1(h)\n",
    "        weights = F.softmax(params.narrow(-1, 0, num_clusters), dim=-1)\n",
    "        mu_1 = params.narrow(-1, num_clusters, num_clusters)\n",
    "        mu_2 = params.narrow(-1, 2*num_clusters, num_clusters)\n",
    "        log_sigma_1 = params.narrow(-1, 3*num_clusters, num_clusters)\n",
    "        log_sigma_2 = params.narrow(-1, 4*num_clusters, num_clusters)\n",
    "        p = self.tanh(params.narrow(-1, 5*num_clusters, num_clusters))\n",
    "        end = F.sigmoid(params.narrow(-1, 6*num_clusters, 1))\n",
    "        \n",
    "        return end, weights, mu_1, mu_2, log_sigma_1, log_sigma_2, p, (h1_n, c1_n), (h2_n, c2_n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training objective\n",
    "def log_likelihood(end, weights, mu_1, mu_2, log_sigma_1, log_sigma_2, p, x, masks):\n",
    "    timesteps = x.shape[1]\n",
    "    x_0 = x.narrow(-1,0,1)\n",
    "    x_1 = x.narrow(-1,1,1)\n",
    "    x_2 = x.narrow(-1,2,1)\n",
    "    end_loglik = (x_0*end + (1-x_0)*(1-end)).log()\n",
    "    const = 1E-20\n",
    "    pi_term = torch.Tensor([2*np.pi])\n",
    "    if cuda:\n",
    "        pi_term = pi_term.cuda()\n",
    "    pi_term = -Variable(pi_term, requires_grad = False).log()\n",
    "    z = (x_1 - mu_1)**2/(log_sigma_1.exp()**2)\\\n",
    "        + ((x_2 - mu_2)**2/(log_sigma_2.exp()**2)) \\\n",
    "        - 2*p*(x_1-mu_1)*(x_2-mu_2)/((log_sigma_1 + log_sigma_2).exp())\n",
    "    mog_lik1 =  pi_term -log_sigma_1 - log_sigma_2 - 0.5*((1-p**2).log())\n",
    "    mog_lik2 = z/(2*(1-p**2))\n",
    "    mog_loglik = ((weights.log() + (mog_lik1 - mog_lik2)).exp().sum(dim=-1)).log()\n",
    "    \n",
    "    return (end_loglik*masks).sum() + ((mog_loglik+const)*masks).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decay_learning_rate(optimizer, decay = 1.01):\n",
    "    state_dict = optimizer.state_dict()\n",
    "    lr = state_dict['param_groups'][0]['lr']\n",
    "    lr /= decay\n",
    "    for param_group in state_dict['param_groups']:\n",
    "        param_group['lr'] = lr\n",
    "    optimizer.load_state_dict(state_dict)\n",
    "    return optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_checkpoint(epoch, model, validation_loss, optimizer, filename='best.pt'):\n",
    "    checkpoint=({'epoch': epoch+1,\n",
    "    'model': model.state_dict(),\n",
    "    'validation_loss': validation_loss,\n",
    "    'optimizer' : optimizer.state_dict()\n",
    "    })\n",
    "    torch.save(checkpoint, filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LSTM()\n",
    "if cuda:\n",
    "    model = model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 10\n",
    "optimizer = optim.Adam([\n",
    "                {'params':model.parameters()},\n",
    "            ], lr=init_lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training\n",
    "h1_init, c1_init = torch.zeros((1,bsize,cell_size)), torch.zeros((1,bsize,cell_size))\n",
    "h2_init, c2_init = torch.zeros((1,bsize,cell_size)), torch.zeros((1,bsize,cell_size))\n",
    "\n",
    "h1_init2, c1_init2 = torch.zeros((1, bsize,cell_size)),\\\n",
    "                    torch.zeros((1,bsize,cell_size))\n",
    "h2_init2, c2_init2 = torch.zeros((1, bsize,cell_size)),\\\n",
    "                    torch.zeros((1,bsize,cell_size))\n",
    "if cuda:\n",
    "    h1_init = h1_init.cuda()\n",
    "    c1_init = c1_init.cuda()\n",
    "    h2_init = h2_init.cuda()\n",
    "    c2_init = c2_init.cuda()\n",
    "    h1_init2 = h1_init2.cuda()\n",
    "    c1_init2 = c1_init2.cuda()\n",
    "    h2_init2 = h2_init2.cuda()\n",
    "    c2_init2 = c2_init2.cuda()\n",
    "h1_init, c1_init = Variable(h1_init), Variable(c1_init)\n",
    "h2_init, c2_init = Variable(h2_init), Variable(c2_init)\n",
    "h1_init2, c1_init2 = Variable(h1_init2), Variable(c1_init2)\n",
    "h2_init2, c2_init2 = Variable(h2_init2), Variable(c2_init2)\n",
    "\n",
    "\n",
    "# t_loss = []\n",
    "# v_loss = []\n",
    "# best_validation_loss = 1E10\n",
    "\n",
    "start_time = time.time()\n",
    "# for epoch in range(epochs):\n",
    "for epoch in range(40,50):\n",
    "    train_loss =0\n",
    "    for batch_idx, (data, masks, onehots) in enumerate(train_loader):\n",
    "        #step_back = torch.cat([zero_tensor, data.narrow(1,0,timesteps-1)], 1)\n",
    "        step_back = data.narrow(1,0,timesteps)\n",
    "        x = Variable(step_back, requires_grad=False)\n",
    "        masks = Variable(masks, requires_grad=False)\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        end, weights, mu_1, mu_2, log_sigma_1, log_sigma_2, p , prev, prev2 = model(x, (h1_init, c1_init), \\\n",
    "                                                                                    (h2_init, c2_init))\n",
    "        data = data.narrow(1,1,timesteps)\n",
    "        y = Variable(data, requires_grad=False)\n",
    "        loss = -log_likelihood(end, weights, mu_1, mu_2, log_sigma_1, log_sigma_2, p, y, masks)/torch.sum(masks)\n",
    "        loss.backward()\n",
    "        train_loss += loss.data[0]\n",
    "        optimizer.step()\n",
    "        if batch_idx % 6 == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch+1, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader),\n",
    "                loss.data[0]))\n",
    "        \n",
    "        \n",
    "\n",
    "    print('====> Epoch: {} Average train loss: {:.4f}'.format(\n",
    "          epoch+1, train_loss / len(train_loader.dataset)))\n",
    "    t_loss.append(train_loss / len(train_loader.dataset))\n",
    "    \n",
    "    # validation\n",
    "    # prepare validation data\n",
    "    (validation_samples, masks, onehots) = list(enumerate(validation_loader))[0][1]\n",
    "    step_back2 = validation_samples.narrow(1,0,timesteps)\n",
    "    masks = Variable(masks, requires_grad=False)\n",
    "    \n",
    "    x2 = Variable(step_back2, requires_grad=False)\n",
    "    \n",
    "    validation_samples = validation_samples.narrow(1,1,timesteps)\n",
    "    y2 = Variable(validation_samples, requires_grad = False)\n",
    "\n",
    "    end, weights, mu_1, mu_2, log_sigma_1, log_sigma_2, p , prev, prev2= model(x2, (h1_init2, c1_init2), \\\n",
    "                                                                              (h2_init2, c2_init2))\n",
    "    loss = -log_likelihood(end, weights, mu_1, mu_2, log_sigma_1, log_sigma_2, p, y2, masks)/torch.sum(masks)\n",
    "    validation_loss = loss.data[0]\n",
    "    print('====> Epoch: {} Average validation loss: {:.4f}'.format(\n",
    "          epoch+1, validation_loss))\n",
    "    v_loss.append(validation_loss)\n",
    "\n",
    "    if validation_loss < best_validation_loss:\n",
    "        best_validation_loss = validation_loss\n",
    "        save_checkpoint(epoch, model, validation_loss, optimizer)\n",
    "    \n",
    "    # learning rate annealing\n",
    "    # optimizer = decay_learning_rate(optimizer, 1.03)\n",
    "    \n",
    "    # checkpoint model and training\n",
    "    #if (epochs+1)%5 == 0:\n",
    "    filename = 'epoch_{}_800.pt'.format(epoch+1)\n",
    "    save_checkpoint(epoch, model, validation_loss, optimizer, filename)\n",
    "\n",
    "    # testing checkpoints\n",
    "    state = torch.load(filename)\n",
    "    model.load_state_dict(state['model'])\n",
    "    optimizer.load_state_dict(state['optimizer'])\n",
    "    \n",
    "    print('wall time: {}s'.format(time.time()-start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_unconditionally(model, steps=700, random_seed=1):\n",
    "    torch.manual_seed(random_seed)\n",
    "    zero_tensor = torch.zeros((1,1,3))\n",
    "    h1_init, c1_init = torch.zeros((1,1,cell_size)), torch.zeros((1,1,cell_size))\n",
    "    h2_init, c2_init = torch.zeros((1,1,cell_size)), torch.zeros((1,1,cell_size))\n",
    "    if cuda:\n",
    "        zero_tensor = zero_tensor.cuda()\n",
    "        h1_init = h1_init.cuda()\n",
    "        c1_init = c1_init.cuda()\n",
    "        h2_init = h2_init.cuda()\n",
    "        c2_init = c2_init.cuda()\n",
    "    x = Variable(zero_tensor)\n",
    "    h1_init, c1_init = Variable(h1_init), Variable(c1_init)\n",
    "    h2_init, c2_init = Variable(h2_init), Variable(c2_init)\n",
    "    prev = (h1_init, c1_init)\n",
    "    prev2 = (h2_init, c2_init)\n",
    "    \n",
    "    record = []\n",
    "    # greedy but not the right generation\n",
    "    for i in range(steps):        \n",
    "        end, weights, mu_1, mu_2, log_sigma_1, log_sigma_2, p, prev, prev2 = model(x, prev, prev2)\n",
    "        prob_end = end.data[0][0][0]\n",
    "        sample_end = np.random.binomial(1,prob_end)\n",
    "        \n",
    "        sample_index = np.random.choice(range(20),p = weights.data[0][0].cpu().numpy())\n",
    "        mu = np.array([mu_1.data[0][0][sample_index], mu_2.data[0][0][sample_index]])\n",
    "        v1 = log_sigma_1.exp().data[0][0][sample_index]**2\n",
    "        v2 = log_sigma_2.exp().data[0][0][sample_index]**2\n",
    "        c = p.data[0][0][sample_index]*log_sigma_1.exp().data[0][0][sample_index]\\\n",
    "            *log_sigma_2.exp().data[0][0][sample_index]\n",
    "        cov = np.array([[v1,c],[c,v2]])\n",
    "        sample_point = np.random.multivariate_normal(mu, cov)\n",
    "        out = np.insert(sample_point,0,sample_end)\n",
    "        record.append(out)\n",
    "        x = torch.from_numpy(out).type(torch.FloatTensor)\n",
    "        if cuda:\n",
    "            x = x.cuda()\n",
    "        x = Variable(x, requires_grad=False)\n",
    "        x = x.view((1,1,3))\n",
    "    return np.array(record)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def greedy_generate(model, steps=700):\n",
    "    #torch.manual_seed(random_seed)\n",
    "    zero_tensor = torch.zeros((1,1,3))\n",
    "    h1_init, c1_init = torch.zeros((1,1,cell_size)), torch.zeros((1,1,cell_size))\n",
    "    h2_init, c2_init = torch.zeros((1,1,cell_size)), torch.zeros((1,1,cell_size))\n",
    "    if cuda:\n",
    "        zero_tensor = zero_tensor.cuda()\n",
    "        h1_init = h1_init.cuda()\n",
    "        c1_init = c1_init.cuda()\n",
    "        h2_init = h2_init.cuda()\n",
    "        c2_init = c2_init.cuda()\n",
    "    x = Variable(zero_tensor)\n",
    "    h1_init, c1_init = Variable(h1_init), Variable(c1_init)\n",
    "    h2_init, c2_init = Variable(h2_init), Variable(c2_init)\n",
    "    prev = (h1_init, c1_init)\n",
    "    prev2 = (h2_init, c2_init)\n",
    "    \n",
    "    record=[]\n",
    "    for i in range(steps):        \n",
    "        end, weights, mu_1, mu_2, log_sigma_1, log_sigma_2, p , prev, prev2 = model(x, prev, prev2)\n",
    "        prob_end = end.data[0][0][0]\n",
    "        sample_end = np.round(prob_end)\n",
    "        \n",
    "        sample_index = np.argmax(weights.data[0][0].cpu().numpy())\n",
    "        mu = np.array([mu_1.data[0][0][sample_index], mu_2.data[0][0][sample_index]])\n",
    "\n",
    "        out = np.array([sample_end, mu[0], mu[1]])\n",
    "        record.append(out)\n",
    "        x = torch.from_numpy(out).type(torch.FloatTensor)\n",
    "        if cuda:\n",
    "            x = x.cuda()\n",
    "        x = Variable(x, requires_grad=False)\n",
    "        x = x.view((1,1,3))\n",
    "    return np.array(record)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state2 = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test generation 60\n",
    "s = generate_unconditionally(model,700, 20)\n",
    "plot_stroke(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test generation 60\n",
    "g = greedy_generate(model)\n",
    "g[650][0]=1\n",
    "plot_stroke(g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def control_generate(model, steps=700, random_seed=1, temp=.9):\n",
    "    torch.manual_seed(random_seed)\n",
    "    zero_tensor = torch.zeros((1,1,3))\n",
    "    h1_init, c1_init = torch.zeros((1,1,cell_size)), torch.zeros((1,1,cell_size))\n",
    "    h2_init, c2_init = torch.zeros((1,1,cell_size)), torch.zeros((1,1,cell_size))\n",
    "    if cuda:\n",
    "        zero_tensor = zero_tensor.cuda()\n",
    "        h1_init = h1_init.cuda()\n",
    "        c1_init = c1_init.cuda()\n",
    "        h2_init = h2_init.cuda()\n",
    "        c2_init = c2_init.cuda()\n",
    "    x = Variable(zero_tensor)\n",
    "    h1_init, c1_init = Variable(h1_init), Variable(c1_init)\n",
    "    h2_init, c2_init = Variable(h2_init), Variable(c2_init)\n",
    "    prev = (h1_init, c1_init)\n",
    "    prev2 = (h2_init, c2_init)\n",
    "    \n",
    "    record = []\n",
    "    # greedy but not the right generation\n",
    "    for i in range(steps):        \n",
    "        if np.random.random() < temp:\n",
    "            end, weights, mu_1, mu_2, log_sigma_1, log_sigma_2, p , prev, prev2 = model(x, prev, prev2)\n",
    "            prob_end = end.data[0][0][0]\n",
    "            sample_end = np.round(prob_end)\n",
    "\n",
    "            sample_index = np.argmax(weights.data[0][0].cpu().numpy())\n",
    "            mu = np.array([mu_1.data[0][0][sample_index], mu_2.data[0][0][sample_index]])\n",
    "\n",
    "            out = np.array([sample_end, mu[0], mu[1]])\n",
    "            record.append(out)\n",
    "            x = torch.from_numpy(out).type(torch.FloatTensor)\n",
    "            if cuda:\n",
    "                x = x.cuda()\n",
    "            x = Variable(x, requires_grad=False)\n",
    "            x = x.view((1,1,3))\n",
    "        \n",
    "        else:\n",
    "            end, weights, mu_1, mu_2, log_sigma_1, log_sigma_2, p, prev, prev2 = model(x, prev, prev2)\n",
    "            prob_end = end.data[0][0][0]\n",
    "            sample_end = np.random.binomial(1,prob_end)\n",
    "\n",
    "            sample_index = np.random.choice(range(20),p = weights.data[0][0].cpu().numpy())\n",
    "            mu = np.array([mu_1.data[0][0][sample_index], mu_2.data[0][0][sample_index]])\n",
    "            v1 = log_sigma_1.data[0][0][sample_index]**2\n",
    "            v2 = log_sigma_2.data[0][0][sample_index]**2\n",
    "            c = p.data[0][0][sample_index]*log_sigma_1.data[0][0][sample_index]*log_sigma_2.data[0][0][sample_index]\n",
    "            cov = np.array([[v1,c],[c,v2]])\n",
    "            sample_point = np.random.multivariate_normal(mu, cov)\n",
    "            out = np.insert(sample_point,0,sample_end)\n",
    "            record.append(out)\n",
    "            x = torch.from_numpy(out).type(torch.FloatTensor)\n",
    "            if cuda:\n",
    "                x = x.cuda()\n",
    "            x = Variable(x, requires_grad=False)\n",
    "            x = x.view((1,1,3))\n",
    "    return np.array(record)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test generation 30\n",
    "s = generate_unconditionally(200,42)\n",
    "plot_stroke(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test generation 30\n",
    "g = greedy_generate()\n",
    "g[650][0]=1\n",
    "plot_stroke(g)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = control_generate(model)\n",
    "plot_stroke(c)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
