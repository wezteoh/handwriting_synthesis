{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import sys "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pytorch modules\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_strokes = np.load('data/train_strokes.npy')\n",
    "train_texts = np.load('data/train_texts.npy')\n",
    "validation_strokes = np.load('data/validation_strokes.npy')\n",
    "validation_texts = np.load('data/validation_texts.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_train_strokes = []\n",
    "new_train_texts = []\n",
    "new_validation_strokes = []\n",
    "new_validation_texts = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "# only train data with length at most 800 for faster training\n",
    "for _ in range(len(train_strokes)):\n",
    "    if len(train_strokes[_]) <= 801:\n",
    "        new_train_strokes.append(train_strokes[_])\n",
    "        new_train_texts.append(train_texts[_])\n",
    "    else:\n",
    "        new_validation_strokes.append(train_strokes[_])\n",
    "        new_validation_texts.append(train_texts[_])\n",
    "for _ in range(len(validation_strokes)):\n",
    "    if len(validation_strokes[_]) <= 801:\n",
    "        new_train_strokes.append(validation_strokes[_])\n",
    "        new_train_texts.append(validation_texts[_])\n",
    "    else:\n",
    "        new_validation_strokes.append(validation_strokes[_])\n",
    "        new_validation_texts.append(validation_texts[_])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pad with zeros and build masks\n",
    "train_masks = np.zeros((len(new_train_strokes),800))\n",
    "for i in range(len(new_train_strokes)):\n",
    "    train_masks[i][0:len(new_train_strokes[i])-1] = 1\n",
    "    new_train_strokes[i] = np.vstack([new_train_strokes[i], np.zeros((801-len(new_train_strokes[i]), 3))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pad with zeros and build masks\n",
    "validation_masks = np.zeros((len(new_validation_strokes),1200))\n",
    "for i in range(len(new_validation_strokes)):\n",
    "    validation_masks[i][0:len(new_validation_strokes[i])-1] = 1\n",
    "    new_validation_strokes[i] = np.vstack([new_validation_strokes[i], \n",
    "                                           np.zeros((1201-len(new_validation_strokes[i]), 3))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('train_strokes_800', np.stack(new_train_strokes))\n",
    "np.save('train_masks_800', train_masks)\n",
    "np.save('validation_strokes_800', np.stack(new_validation_strokes))\n",
    "np.save('validation_masks_800', validation_masks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert each text sentence to an array\n",
    "char_list = ' ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz,.\"\\'?-'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "char_to_code = {}\n",
    "code_to_char = {}\n",
    "c = 0\n",
    "for _ in char_list:\n",
    "    char_to_code[_] = c\n",
    "    code_to_char[c] = _\n",
    "    c += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len = np.max(np.array([len(a) for a in new_validation_texts]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_onehot_800 = []\n",
    "for t in new_train_texts:\n",
    "    onehots = np.zeros((max_text_len, len(char_to_code)+1))\n",
    "    for _ in range(len(t)):\n",
    "        try:\n",
    "            onehots[_][char_to_code[t[_]]] = 1\n",
    "        except:\n",
    "            onehots[_][-1] = 1\n",
    "    for _ in range(len(t), max_text_len):\n",
    "        onehots[_][-1] = 1\n",
    "    train_onehot_800.append(onehots)\n",
    "train_onehot_800 = np.stack(train_onehot_800)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_onehot_800 = []\n",
    "for t in new_train_texts:\n",
    "    onehots = np.zeros((max_text_len, len(char_to_code)+1))\n",
    "    for _ in range(len(t)):\n",
    "        try:\n",
    "            onehots[_][char_to_code[t[_]]] = 1\n",
    "        except:\n",
    "            onehots[_][-1] = 1\n",
    "    for _ in range(len(t), max_text_len):\n",
    "        onehots[_][-1] = 1\n",
    "    validation_onehot_800.append(onehots)\n",
    "validation_onehot_800 = np.stack(validation_onehot_800)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('train_onehot_800', train_onehot_800)\n",
    "np.save('validation_onehot_800', validation_onehot_800)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
